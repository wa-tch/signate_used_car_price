{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -U scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import unicodedata\n","from pathlib import Path\n","import optuna\n","import catboost as cb\n","import lightgbm as lgb\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","from category_encoders import CountEncoder\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import mean_absolute_percentage_error\n","from sklearn.model_selection import KFold\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import TargetEncoder\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import sklearn\n","sklearn.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["INPUT_DIR = Path(\"/kaggle/input/signate-used-car-price/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","train_df = pd.read_csv(INPUT_DIR / \"train.csv\")\n","test_df = pd.read_csv(INPUT_DIR / \"test.csv\")\n","\n","sub_df = pd.read_csv(INPUT_DIR / \"submit_sample.csv\", names=[\"id\", \"price\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# train_df.query(\"price > 45000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# train_filtered  = train_df[train_df['price'] <= 50000].reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_df.isnull().sum()\n","# train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class PreProcessTransformer(TransformerMixin, BaseEstimator):\n","    def __init__(self, threshold_year=2000):\n","        self.threshold_year = threshold_year\n","\n","    def fit(self, X, y=None):#訓練データに対してのみ呼び出されるもので、データに基づいて変換器を訓練する役割\n","        self.avg_price_per_year = X[X['year'] > self.threshold_year].groupby('year')['price'].mean().to_dict()\n","        return self\n","\n","    def transform(self, X):#任意のデータセットに対して呼び出されるもので、fitで計算されたパラメータを使用してデータを変換する役割\n","        # regionからstateを推測して欠損値を補完する関数\n","        def fill_state_from_region(df):\n","            region_state = {region: {} for region in df['region'].unique()}\n","            for row, value in df.iterrows():\n","                if not pd.isna(value['state']):\n","                    if value['state'] not in region_state[value['region']]:\n","                        region_state[value['region']][value['state']] = 1\n","                    else:\n","                        region_state[value['region']][value['state']] += 1\n","\n","            for region, state_dict in region_state.items():\n","                if len(state_dict) > 1 or state_dict == {}:\n","                    region_state[region] = pd.NA\n","                else:\n","                    region_state[region] = list(state_dict.keys())[0]\n","\n","            df['state'] = [region_state[region] if pd.isna(state) else state for region, state in zip(df['region'], df['state'])]\n","            \n","            region_to_state = {\n","                'northwest KS': 'ks',\n","                'southern WV': 'wv',\n","                'ashtabula': 'oh',\n","            }\n","            # 補完できなかったstateを決定\n","            df['state'] = [\n","                region_to_state.get(region, state) if pd.isna(state) else state\n","                for region, state in zip(df['region'], df['state'])\n","            ]\n","\n","            return df\n","        \n","        X = fill_state_from_region(X)\n","        \n","        # 年度ごとの平均価格を追加\n","        # 訓練データで計算した年度ごとの平均価格を使用\n","#         X['yearly_avg_price'] = X['year'].apply(lambda x: self.avg_price_per_year.get(x, np.nan) if x > self.threshold_year else np.nan)\n","\n","        \n","        # cylinderから数値を取り出す\n","        X[\"cylinders\"] = X[\"cylinders\"].astype(str).str.extract(\"(\\d+)\").astype(\"float32\")\n","\n","        # sizeの表記揺れを修正\n","        X[\"size\"] = X[\"size\"].str.replace(\"ー\", \"-\").astype(str)\n","        X[\"size\"] = X[\"size\"].str.replace(\"−\", \"-\").astype(str)\n","\n","        # manufacturerの表記揺れを修正\n","        X[\"manufacturer\"] = X[\"manufacturer\"].apply(\n","            lambda x: unicodedata.normalize(\"NFKC\", x).lower()\n","        )\n","        manufacturer_mapping = {\n","        \"toyotа\": \"toyota\",\n","        \"subαru\": \"subaru\",\n","        \"niѕsan\": \"nissan\",\n","        \"nisѕan\": \"nissan\",\n","        \"sαturn\": \"saturn\",\n","        \"lexuѕ\": \"lexus\",\n","        \"vоlkswagen\": \"volkswagen\",\n","        \"аcura\": \"acura\",\n","        \"ᴄhrysler\": \"chrysler\",\n","        \"land rover\": \"rover\" # 仮に\"land rover\"を\"roover\"に統一する場合\n","        }\n","\n","        X[\"manufacturer\"] = X[\"manufacturer\"].replace(manufacturer_mapping)\n","\n","\n","        # yearが2500年以降のものはおかしいので2000年代に置換\n","        err_idx = X.query(\"year >= 2500\").index\n","        X.loc[err_idx, \"year\"] = X.loc[err_idx, \"year\"].apply(lambda x: x - 1000)\n","\n","        \n","        # 走行距離が負はおかしいので0に\n","        err_idx_odo = X.query(\"odometer < 0\").index\n","        X.loc[err_idx_odo, \"odometer\"] = X.loc[err_idx_odo, \"odometer\"].apply(lambda x: 0)\n","        \n","#         # 走行距離が400000超えはおかしいので一桁落とす\n","#         err_idx_odo = X.query(\"odometer > 400000\").index\n","#         X.loc[err_idx_odo, \"odometer\"] = X.loc[err_idx_odo, \"odometer\"] // 10\n","\n","#         # 走行距離が400000超えはおかしいので、該当する行を削除する\n","#         X = X.query(\"odometer <= 400000\")\n","\n","        \n","        # 走行距離 / 製造年\n","        X[\"odometer_per_year\"] = X[\"odometer\"] / (2023 - X[\"year\"])\n","        \n","        \n","        return X\n","\n","\n","class RankTransformer(TransformerMixin, BaseEstimator):\n","    \"\"\"keyの中でvalueが何番目のものか\"\"\"\n","\n","    def __init__(self, key: str, value: str):\n","        self.key = key\n","        self.value = value\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        X_new = X.copy()\n","        X_new[self.key] = X_new.groupby(self.key)[self.value].rank(method=\"dense\")\n","        return X_new[self.get_feature_names_out()]\n","\n","    def get_feature_names_out(self, input_features=None):\n","        return [self.key]\n","    \n","    \n","class OriginalTransformer(TransformerMixin, BaseEstimator):\n","    \"\"\"数値特徴はそのまま、カテゴリ特徴はcategory型に変換\"\"\"\n","\n","    def __init__(self, numeric_cols, categorical_cols):\n","        self.numeric_cols = numeric_cols\n","        self.categorical_cols = categorical_cols\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        X_new = X.copy()\n","\n","        # 数値変数\n","        X_new[self.numeric_cols] = X_new[self.numeric_cols].astype(\"float32\")\n","\n","        # カテゴリ変数\n","        X_new[self.categorical_cols] = X_new[self.categorical_cols].astype(\"category\")\n","\n","        return X_new[self.get_feature_names_out()]\n","\n","    def get_feature_names_out(self):\n","        return self.numeric_cols + self.categorical_cols\n","    \n","    \n","    \n","# class CountTransformer(TransformerMixin, BaseEstimator):\n","#     \"\"\"CountEncoder\"\"\"\n","\n","#     def __init__(self):\n","#         pass\n","\n","#     def fit(self, X, y=None):\n","#         self.ce = CountEncoder(cols=X.columns.tolist(), handle_unknown=0)\n","#         self.ce.fit(X)\n","#         return self\n","\n","#     def transform(self, X):\n","#         return self.ce.transform(X)\n","\n","#     def get_feature_names_out(self, input_features=None):\n","#         return input_features\n","class CountTransformer(TransformerMixin, BaseEstimator):\n","    \"\"\"CountEncoder\"\"\"\n","\n","    def __init__(self, categorical_cols):  # この行を変更\n","        self.categorical_cols = categorical_cols\n","\n","    def fit(self, X, y=None):\n","        self.ce = CountEncoder(cols=self.categorical_cols, handle_unknown=0)\n","        self.ce.fit(X[self.categorical_cols])  # この行を変更\n","        return self\n","\n","    def transform(self, X):\n","        return self.ce.transform(X[self.categorical_cols])  # この行を変更\n","\n","    def get_feature_names_out(self, input_features=None):\n","        return self.categorical_cols\n","\n","    \n","    \n","    \n","class AggTransformer(TransformerMixin, BaseEstimator):\n","    \"\"\"集約特徴量\"\"\"\n","\n","    def __init__(self, key, numeric_cols, agg_func: dict):\n","        self.key = key\n","        self.numeric_cols = numeric_cols\n","        self.agg_func = agg_func\n","\n","    def fit(self, X, y=None):\n","        X = X.copy()\n","        X[self.key] = X[self.key].astype(\"category\")\n","        self.agg_df = X.groupby(self.key)[self.numeric_cols].agg(self.agg_func)\n","        self.agg_df.columns = [f\"{col}_{func}\" for col, func in self.agg_df.columns.tolist()]\n","\n","        return self\n","\n","    def transform(self, X):\n","        X_new = pd.merge(X, self.agg_df, on=self.key, how=\"left\")\n","        return X_new[self.get_feature_names_out()]\n","\n","    def get_feature_names_out(self, input_features=None):\n","        return self.agg_df.columns.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# numeric_cols = [\"year\", \"odometer\", \"odometer_per_year\"]\n","numeric_cols = [\"year\", \"odometer\"]\n","categorical_cols = [\n","#      'region',\n","    \"cylinders\",\n","    \"manufacturer\",\n","    \"condition\",\n","    \"fuel\",\n","    \"title_status\",\n","    \"transmission\",\n","    \"drive\",\n","    \"size\",\n","    \"type\",\n","    \"paint_color\",\n","    \"state\",\n","]\n","\n","ct = ColumnTransformer(\n","    transformers=[\n","        (\n","            \"ori\",\n","            OriginalTransformer(numeric_cols, categorical_cols),\n","            categorical_cols + numeric_cols,\n","        ),\n","#         (\n","#             \"cnt\",\n","#             CountTransformer(categorical_cols),  # この行を追加\n","#             categorical_cols  # カテゴリカル変数に対して適用\n","#         ),\n","        (\n","            \"tgt\",\n","            TargetEncoder(target_type=\"continuous\", random_state=88),\n","            [\n","                'region',\n","                \"cylinders\",\n","                \"manufacturer\",\n","                \"condition\",\n","                \"fuel\",\n","                \"title_status\",\n","                \"transmission\",\n","                \"drive\",\n","                \"size\",\n","                \"type\",\n","                \"paint_color\",\n","                \"state\",\n","            ],\n","        ),\n","        *[\n","            (f\"agg_{key}\", AggTransformer(key, [\"odometer\"], {\"median\"}), [key] + [\"odometer\"])\n","            for key in [\n","#                 \"cylinders\",\n","                \"manufacturer\",\n","                \"condition\",\n","                \"paint_color\",\n","#                 \"fuel\",\n","#                 \"title_status\",\n","#                 \"transmission\",\n","#                 \"drive\",\n","#                 \"size\",\n","#                 \"type\",\n","#                 \"state\",\n","#                 \"year\"\n","            ]\n","        ],\n","        \n","    ],\n","    verbose=True,\n",")\n","ct.set_output(transform=\"pandas\")\n","pipe = Pipeline(\n","    steps=[\n","        (\"preprocess\", PreProcessTransformer(threshold_year=2000)),\n","        (\"ct\", ct),\n","    ]\n",")\n","# train_df_filtered = train_df[train_df['price'] <= 50000].reset_index(drop=True)\n","# train_feat_df_filtered = pipe.fit_transform(train_df_filtered, train_df_filtered[\"price\"])\n","# 訓練データに対してパイプラインを適合させる\n","pipe.fit(train_df, train_df['price'])\n","\n","train_feat_df = pipe.transform(train_df)\n","test_feat_df = pipe.transform(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_feat_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T15:45:28.784040Z","iopub.status.busy":"2023-08-21T15:45:28.783456Z","iopub.status.idle":"2023-08-21T15:45:28.793354Z","shell.execute_reply":"2023-08-21T15:45:28.791990Z","shell.execute_reply.started":"2023-08-21T15:45:28.783996Z"},"trusted":true},"outputs":[],"source":["train_feat_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T10:48:11.746635Z","iopub.status.busy":"2023-08-21T10:48:11.746255Z","iopub.status.idle":"2023-08-21T10:48:11.787266Z","shell.execute_reply":"2023-08-21T10:48:11.786503Z","shell.execute_reply.started":"2023-08-21T10:48:11.746606Z"},"trusted":true},"outputs":[],"source":["# sns.histplot(data = train_feat_df[\"ori__odometer\"])\n","# train_feat_df.isnull().sum()\n","# train_feat_df.info()\n","# train_feat_df_filtered.info()\n","train_feat_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T10:30:29.981411Z","iopub.status.busy":"2023-08-21T10:30:29.981005Z","iopub.status.idle":"2023-08-21T10:30:30.002219Z","shell.execute_reply":"2023-08-21T10:30:30.001488Z","shell.execute_reply.started":"2023-08-21T10:30:29.981378Z"},"trusted":true},"outputs":[],"source":["# train_feat_df.query(\"ori__odometer > 40000 \")\n","test_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-14T15:49:17.867224Z","iopub.status.busy":"2023-08-14T15:49:17.866729Z","iopub.status.idle":"2023-08-14T15:49:17.874021Z","shell.execute_reply":"2023-08-14T15:49:17.872454Z","shell.execute_reply.started":"2023-08-14T15:49:17.867186Z"},"trusted":true},"outputs":[],"source":["# test_feat_df[\"ori__manufacturer\"].unique().value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-14T15:49:18.242697Z","iopub.status.busy":"2023-08-14T15:49:18.241340Z","iopub.status.idle":"2023-08-14T15:49:18.255960Z","shell.execute_reply":"2023-08-14T15:49:18.254148Z","shell.execute_reply.started":"2023-08-14T15:49:18.242520Z"},"trusted":true},"outputs":[],"source":["train_feat_df[\"ori__title_status\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T14:31:44.607607Z","iopub.status.busy":"2023-08-21T14:31:44.607088Z","iopub.status.idle":"2023-08-21T14:31:44.632284Z","shell.execute_reply":"2023-08-21T14:31:44.630817Z","shell.execute_reply.started":"2023-08-21T14:31:44.607561Z"},"trusted":true},"outputs":[],"source":["train_feat_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas_profiling\n","train_feat_df.profile_report()"]},{"cell_type":"markdown","metadata":{},"source":["**考察**\n","\n","odometer　負の値を0に、極端にデカい値は放置\n","\n","regionからstateの決定は意味ありそう\n","\n","ori__fuel has 1239 (4.5%) missing values\tMissing\n","\n","ori__title_status has 456 (1.7%) missing values\tMissing\n","\n","ori__type has 456 (1.7%) missing values\tMissing\n","\n","ori__year is highly skewed (γ1 = 20.13225365)\n","\n","count codingの検討、ダメだった\n","\n","optunaでcatと統合、ダメだった\n","\n","交差検証モデル予測の平均の方が精度が高い\n","\n","最良モデルを再現して年平均価格の評価をすべき、1980年以前ではなく2000年以前を欠損に変更\n","\n","対数変換、地理的ラベルの検討"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas_profiling\n","test_feat_df.profile_report()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T14:13:07.022640Z","iopub.status.busy":"2023-08-21T14:13:07.022208Z","iopub.status.idle":"2023-08-21T14:13:07.056915Z","shell.execute_reply":"2023-08-21T14:13:07.055435Z","shell.execute_reply.started":"2023-08-21T14:13:07.022607Z"},"trusted":true},"outputs":[],"source":["def get_cv(df, n_fold=5):\n","    kf = KFold(n_splits=n_fold, shuffle=True, random_state=71)\n","    return list(kf.split(df))\n","\n","# ref: https://www.guruguru.science/competitions/16/discussions/185c7dc6-5e3a-49c6-9c30-41bf007cc694/\n","def fit_lgbm(X, y, cv, categorical_cols: list = None, params: dict = None, verbose: int = 50):\n","    # パラメータがないときは、空の dict で置き換える\n","    if params is None:\n","        params = {}\n","\n","    models = []\n","    n_records = len(X)\n","    oof_pred = np.zeros((n_records,), dtype=np.float32)\n","\n","    for i, (idx_train, idx_valid) in enumerate(cv):\n","        x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n","        x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n","\n","        model = lgb.LGBMRegressor(**params)\n","        model.fit(\n","            x_train,\n","            y_train,\n","            categorical_feature=categorical_cols,\n","            eval_set=[(x_valid, y_valid)],\n","            callbacks=[lgb.early_stopping(100, verbose=verbose)],\n","        )\n","        pred_i = model.predict(x_valid)\n","        oof_pred[idx_valid] = pred_i\n","        models.append(model)\n","        score = mean_absolute_percentage_error(y_valid, pred_i)\n","        print(f\" - fold{i + 1} - {score:.4f}\")\n","\n","    score = mean_absolute_percentage_error(y, oof_pred)\n","\n","    print(\"=\" * 50)\n","    print(f\"FINISHI: Whole Score: {score:.4f}\")\n","    return score, oof_pred, models\n","\n","def convert_categorical_features(df, categorical_cols):\n","    if categorical_cols:\n","        for col in categorical_cols:\n","            df[col] = df[col].astype(str)\n","    return df\n","\n","def fit_cat(X, y, cv, categorical_cols: list = None, params: dict = None, verbose = False):\n","    if params is None:\n","        params = {}\n","    X = convert_categorical_features(X, categorical_cols)\n","            \n","    models = []\n","    n_records = len(X)\n","    oof_pred = np.zeros((n_records,), dtype=np.float32)\n","\n","    for i, (idx_train, idx_valid) in enumerate(cv):\n","        x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n","        x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n","        \n","        train_data = cb.Pool(data=x_train, label=y_train, cat_features=categorical_cols)\n","        valid_data = cb.Pool(data=x_valid, label=y_valid, cat_features=categorical_cols)\n","\n","        model = cb.CatBoostRegressor(**params)\n","        model.fit(\n","            x_train,\n","            y_train,\n","            cat_features=categorical_cols,\n","            eval_set=[(x_valid, y_valid)],\n","            use_best_model=True,\n","            early_stopping_rounds=100,\n","#             verbose=verbose,\n","        \n","            \n","        )\n","        pred_i = model.predict(x_valid)\n","        oof_pred[idx_valid] = pred_i\n","        models.append(model)\n","        score = mean_absolute_percentage_error(y_valid, pred_i)\n","        print(f\" - fold{i + 1} - {score:.4f}\")\n","\n","    score = mean_absolute_percentage_error(y, oof_pred)\n","\n","    print(\"=\" * 50)\n","    print(f\"FINISHI: Whole Score: {score:.4f}\")\n","    return score, oof_pred, models\n","\n","# optunaを使ったパラメータチューニング\n","def tuning_cat(X, y, cv, categorical_cols):\n","    def objective(trial):\n","        params = {\n","#         'iterations': trial.suggest_int('iterations', 100, 2000),\n","        'iterations' : 10000,\n","#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n","        'learning_rate' : 0.02,\n","        'depth': trial.suggest_int('depth', 4, 10),\n","        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n","#         'border_count': trial.suggest_int('border_count', 32, 255),\n","        'loss_function': 'MAPE',\n","        'eval_metric': 'MAPE',\n","        'verbose': False,\n","#         'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n","#         'od_wait': trial.suggest_int('od_wait', 10, 50)\n","        \n","        }\n","\n","        score, _, _ = fit_cat(X, y, cv=cv, categorical_cols=categorical_cols, params=params, verbose=False)\n","        return score\n","\n","    study = optuna.create_study(direction='minimize')\n","    study.optimize(objective, n_trials=10)\n","    return study.best_trial.params\n","\n","\n","# optunaを使ったパラメータチューニング\n","def tuning_lgbm(train_feat_df, train_df, cv):\n","    def objective(trial):\n","        max_depth = trial.suggest_int(\"max_depth\", 4, 10)\n","        num_leaves = trial.suggest_int(\"num_leaves\", 2, 2**max_depth)\n","        colsample_bytree = trial.suggest_uniform(\"colsample_bytree\", 0.1, 1.0)\n","        subsample = trial.suggest_uniform(\"subsample\", 0.1, 1.0)\n","\n","        params = {\n","            \"objective\": \"mape\",\n","            \"n_estimators\": 10000,\n","            \"learning_rate\": 0.05,\n","            \"max_depth\": max_depth,\n","            \"num_leaves\": num_leaves,\n","            \"colsample_bytree\": colsample_bytree,\n","            \"subsample\": subsample,\n","            \"metric\": \"mape\",\n","            \"importance_type\": \"gain\",\n","            \"random_state\": 88,\n","        }\n","\n","        score, _, _ = fit_lgbm(\n","            train_feat_df, train_df[\"price\"], cv=cv, categorical_cols=[], params=params, verbose=-1\n","        )\n","        return score\n","\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(objective, n_trials=100)\n","    print(\"Number of finished trials:\", len(study.trials))\n","    print(\"Best trial:\", study.best_trial.params)\n","    return study.best_trial.params"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T14:13:07.777732Z","iopub.status.busy":"2023-08-21T14:13:07.777229Z","iopub.status.idle":"2023-08-21T14:13:07.794517Z","shell.execute_reply":"2023-08-21T14:13:07.792512Z","shell.execute_reply.started":"2023-08-21T14:13:07.777694Z"},"trusted":true},"outputs":[],"source":["def visualize_importance(models, feat_train_df):\n","    feature_importance_df = pd.DataFrame()\n","    for i, model in enumerate(models):\n","        _df = pd.DataFrame()\n","        _df[\"feature_importance\"] = model.feature_importances_\n","        _df[\"column\"] = feat_train_df.columns\n","        _df[\"fold\"] = i + 1\n","        feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n","\n","    order = (\n","        feature_importance_df.groupby(\"column\")\n","        .sum()[[\"feature_importance\"]]\n","        .sort_values(\"feature_importance\", ascending=False)\n","        .index[:50]\n","    )\n","\n","    fig, ax = plt.subplots(figsize=(12, max(6, len(order) * 0.25)))\n","    sns.boxenplot(\n","        data=feature_importance_df,\n","        x=\"feature_importance\",\n","        y=\"column\",\n","        order=order,\n","        ax=ax,\n","        palette=\"viridis\",\n","        orient=\"h\",\n","    )\n","    ax.tick_params(axis=\"x\", rotation=90)\n","    ax.set_title(\"Importance\")\n","    ax.grid()\n","    fig.tight_layout()\n","    return fig, ax\n","\n","\n","def visualize_oof_gt(oof, gt):\n","    fig, ax = plt.subplots(figsize=(8, 6))\n","    ax.scatter(oof, gt, alpha=0.5)\n","    gt_max = gt.max()\n","    ax.plot(np.arange(0, gt_max), np.arange(0, gt_max), color=\"red\", alpha=0.5, linestyle=\"--\")\n","    ax.set_xlabel(\"Out Of Fold\")\n","    ax.set_ylabel(\"Ground Truth\")\n","    ax.grid()\n","    ax.legend()\n","    fig.tight_layout()\n","\n","    fig, ax\n","\n","\n","def visualize_oof_pred(oof, pred):\n","    fig, ax = plt.subplots(figsize=(8, 6))\n","\n","    bins = 100\n","    ax.hist(pred, bins=bins, density=True, alpha=0.5, label=\"Test\")\n","    ax.hist(oof, bins=bins, density=True, alpha=0.5, label=\"OutOfFold\")\n","    ax.grid()\n","    ax.legend()\n","    fig.tight_layout()\n","\n","    fig, ax\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T15:07:01.617273Z","iopub.status.busy":"2023-08-21T15:07:01.616846Z","iopub.status.idle":"2023-08-21T15:07:01.641357Z","shell.execute_reply":"2023-08-21T15:07:01.640145Z","shell.execute_reply.started":"2023-08-21T15:07:01.617241Z"},"trusted":true},"outputs":[],"source":["train_feat_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T16:00:44.717939Z","iopub.status.busy":"2023-08-21T16:00:44.717490Z","iopub.status.idle":"2023-08-21T16:02:24.368836Z","shell.execute_reply":"2023-08-21T16:02:24.367570Z","shell.execute_reply.started":"2023-08-21T16:00:44.717904Z"},"trusted":true},"outputs":[],"source":["n_fold = 5\n","lgbm_params = {\n","    \"objective\": \"mape\",\n","    \"metrics\": \"mape\",\n","    \"n_estimators\": 10000,\n","    \"learning_rate\": 0.01,\n","    \"max_depth\": 5,\n","    \"num_leaves\": 32,\n","    \"colsample_bytree\": 0.446,\n","    \"subsample\": 0.339,\n","    \"importance_type\": \"gain\",\n","    \"random_state\": 88,\n","}\n","\n","feat_cat_cols = train_feat_df.select_dtypes(include=\"category\").columns.tolist()\n","\n","cv = get_cv(train_feat_df, n_fold=5)\n","\n","# training\n","score_lgbm, oof_lgbm, models_lgbm = fit_lgbm(\n","    train_feat_df,\n","    train_df[\"price\"],\n","    categorical_cols=[],\n","    params=lgbm_params,\n","    cv=cv,\n","    verbose=False,\n",")\n","\n","# inference\n","pred_lgbm = np.array([model.predict(test_feat_df) for model in models_lgbm])\n","pred_lgbm = np.mean(pred_lgbm, axis=0)\n","\n","visualize_importance(models_lgbm, train_feat_df)\n","visualize_oof_gt(oof_lgbm, train_df['price'])\n","visualize_oof_pred(oof_lgbm, pred_lgbm)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T14:48:52.036719Z","iopub.status.busy":"2023-08-21T14:48:52.036197Z","iopub.status.idle":"2023-08-21T14:51:36.184298Z","shell.execute_reply":"2023-08-21T14:51:36.180868Z","shell.execute_reply.started":"2023-08-21T14:48:52.036679Z"},"trusted":true},"outputs":[],"source":["import pandas_profiling\n","train_feat_df.profile_report()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T15:40:58.115811Z","iopub.status.busy":"2023-08-21T15:40:58.115349Z","iopub.status.idle":"2023-08-21T15:43:04.645071Z","shell.execute_reply":"2023-08-21T15:43:04.643893Z","shell.execute_reply.started":"2023-08-21T15:40:58.115778Z"},"trusted":true},"outputs":[],"source":["train_feat_df.profile_report()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T16:04:05.257017Z","iopub.status.busy":"2023-08-21T16:04:05.256566Z","iopub.status.idle":"2023-08-21T16:04:05.384687Z","shell.execute_reply":"2023-08-21T16:04:05.383281Z","shell.execute_reply.started":"2023-08-21T16:04:05.256985Z"},"trusted":true},"outputs":[],"source":["# # submission\n","sub_df[\"price\"] = pred_lgbm\n","sub_df.to_csv(\"submission28.csv\", index=False, header=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T14:14:53.142222Z","iopub.status.busy":"2023-08-21T14:14:53.141728Z","iopub.status.idle":"2023-08-21T14:14:56.374251Z","shell.execute_reply":"2023-08-21T14:14:56.372907Z","shell.execute_reply.started":"2023-08-21T14:14:53.142176Z"},"trusted":true},"outputs":[],"source":["visualize_importance(models_lgbm, train_feat_df)\n","visualize_oof_gt(oof_lgbm, train_df['price'])\n","visualize_oof_pred(oof_lgbm, pred_lgbm)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-14T16:38:39.838075Z","iopub.status.busy":"2023-08-14T16:38:39.837513Z","iopub.status.idle":"2023-08-14T16:38:39.847818Z","shell.execute_reply":"2023-08-14T16:38:39.846093Z","shell.execute_reply.started":"2023-08-14T16:38:39.838024Z"},"trusted":true},"outputs":[],"source":["# def train_and_validate_with_filtered_data(X_filtered, y_filtered, X_full, y_full, cv, params=None, verbose=False):\n","#     models = []\n","#     oof_pred = np.zeros(len(X_full), dtype=np.float32)\n","    \n","#     for i, (idx_train, idx_valid) in enumerate(cv):\n","#         # 除去データで学習\n","#         x_train, y_train = X_filtered, y_filtered\n","#         x_valid, y_valid = X_full.iloc[idx_valid], y_full.iloc[idx_valid]\n","\n","#         model = lgb.LGBMRegressor(**params)\n","#         model.fit(\n","#             x_train,\n","#             y_train,\n","#             eval_set=[(x_valid, y_valid)],\n","#             early_stopping_rounds=100,\n","#             verbose=verbose,\n","#         )\n","        \n","#         pred_i = model.predict(x_valid)\n","#         oof_pred[idx_valid] = pred_i\n","#         models.append(model)\n","#         score = mean_absolute_percentage_error(y_valid, pred_i)\n","#         print(f\" - fold{i + 1} - {score:.4f}\")\n","\n","#     score = mean_absolute_percentage_error(y_full, oof_pred)\n","\n","#     print(\"=\" * 50)\n","#     print(f\"FINISHI: Whole Score: {score:.4f}\")\n","#     return score, oof_pred, models\n","\n","# # # 除去データ\n","# # train_df_filtered = train_df[train_df['price'] <= 50000]\n","# # train_feat_df_filtered = pipe.transform(train_df_filtered)\n","\n","# # # 除去していないデータの前処理\n","# # train_feat_df = pipe.transform(train_df)\n","\n","# # 固定パラメータ\n","# params = {\n","#     \"objective\": \"mape\",\n","#     \"n_estimators\": 1000,\n","#     \"learning_rate\": 0.05,\n","#     \"max_depth\": 6,\n","#     \"num_leaves\": 31,\n","#     \"random_state\": 88,\n","# }\n","# lgbm_params = {\n","#     \"objective\": \"mape\",\n","#     \"metrics\": \"mape\",\n","#     \"n_estimators\": 10000,\n","#     \"learning_rate\": 0.01,\n","#     \"max_depth\": 5,\n","#     \"num_leaves\": 32,\n","#     \"colsample_bytree\": 0.446,\n","#     \"subsample\": 0.339,\n","#     \"importance_type\": \"gain\",\n","#     \"random_state\": 88,\n","# }\n","# # 交差検証\n","# cv = get_cv(train_df)\n","# score_lgbm_filter, oof_lgbm_filter, models_lgbm_filter = train_and_validate_with_filtered_data(train_feat_df_filtered, train_df_filtered[\"price\"], train_feat_df, train_df[\"price\"], cv, params=lgbm_params)\n","\n","# # inference\n","# pred_lgbm_filter = np.array([model.predict(test_feat_df) for model in models_lgbm_filter])\n","# pred_lgbm_filter = np.mean(pred_lgbm_filter, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-14T16:20:21.638897Z","iopub.status.busy":"2023-08-14T16:20:21.638438Z","iopub.status.idle":"2023-08-14T16:20:25.191898Z","shell.execute_reply":"2023-08-14T16:20:25.190740Z","shell.execute_reply.started":"2023-08-14T16:20:21.638864Z"},"trusted":true},"outputs":[],"source":["visualize_importance(models_lgbm_filter, train_feat_df)\n","visualize_oof_gt(oof_lgbm_filter, train_df['price'])\n","visualize_oof_pred(oof_lgbm_filter, pred_lgbm_filter)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-14T16:12:51.235949Z","iopub.status.busy":"2023-08-14T16:12:51.235503Z","iopub.status.idle":"2023-08-14T16:12:51.360199Z","shell.execute_reply":"2023-08-14T16:12:51.359289Z","shell.execute_reply.started":"2023-08-14T16:12:51.235915Z"},"trusted":true},"outputs":[],"source":["# # submission\n","sub_df[\"price\"] = pred_lgbm_filter\n","sub_df.to_csv(\"submission17.csv\", index=False, header=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T14:15:33.310758Z","iopub.status.busy":"2023-08-21T14:15:33.310276Z","iopub.status.idle":"2023-08-21T14:15:54.640014Z","shell.execute_reply":"2023-08-21T14:15:54.638599Z","shell.execute_reply.started":"2023-08-21T14:15:33.310720Z"},"trusted":true},"outputs":[],"source":["def analyze_outliers(n_points=20):\n","    # 予測値と実際の値との差異を計算（MAPE）\n","    error_df = pd.DataFrame()\n","    error_df['prediction'] = oof_lgbm\n","    error_df['true_price'] = train_df.copy()['price']\n","    error_df['percentage_error'] = abs(error_df['true_price'] - error_df['prediction']) / error_df['true_price'] * 100 / 27532\n","\n","    # 差異が大きいデータポイントを取得\n","    large_error_df = error_df.nlargest(n_points, 'percentage_error')\n","\n","    # グラフの幅をデータポイントの数に応じて調整\n","    width = max(15, n_points * 0.05)\n","\n","    # 差異の大きいデータポイントを可視化\n","    plt.figure(figsize=(width, 10))\n","    sns.barplot(x=large_error_df.index, y=large_error_df['percentage_error'], order=large_error_df.index)\n","    plt.xlabel('Index')\n","    plt.ylabel('Percentage Error')\n","    plt.title(f'Top {n_points} Data Points with Largest Prediction Error (MAPE)')\n","    plt.show()\n","\n","    # 誤差が大きい順に指定した個数分のtrain_dfに予測値も追加して返す\n","    top_n_indices = large_error_df.index.values\n","    top_n_train_df = train_df.iloc[top_n_indices]\n","    top_n_train_df['prediction'] = large_error_df['prediction'].values\n","\n","    return top_n_train_df\n","\n","top_n_train_df_with_predictions = analyze_outliers(n_points=2000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T14:21:14.945955Z","iopub.status.busy":"2023-08-21T14:21:14.945489Z","iopub.status.idle":"2023-08-21T14:21:14.974344Z","shell.execute_reply":"2023-08-21T14:21:14.973388Z","shell.execute_reply.started":"2023-08-21T14:21:14.945919Z"},"trusted":true},"outputs":[],"source":["top_n_train_df_with_predictions.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T14:28:25.719220Z","iopub.status.busy":"2023-08-21T14:28:25.718785Z","iopub.status.idle":"2023-08-21T14:28:32.193804Z","shell.execute_reply":"2023-08-21T14:28:32.192238Z","shell.execute_reply.started":"2023-08-21T14:28:25.719178Z"},"trusted":true},"outputs":[],"source":["# 分析したいカテゴリカラム\n","category_columns_to_analyze = [\n","    'manufacturer',\n","    'condition',\n","    'paint_color',\n","    'state',\n","    'size',\n","    'type',\n","    'transmission'\n","    ''\n","    \n","    \n","]\n","\n","# 分析用のデータセットを作成\n","analysis_df = top_n_train_df_with_predictions[category_columns_to_analyze]\n","\n","# 元のデータセットと誤差の大きいデータセットの両方でプロット\n","for col in category_columns_to_analyze:\n","    fig, axes = plt.subplots(1, 2, figsize=(30, 5))\n","    \n","    # 元のデータセットの分布をプロット\n","    sns.countplot(data=train_df, x=col, order=train_df[col].value_counts().index, ax=axes[0])\n","    axes[0].set_title(f'Distribution of {col} in Original Data')\n","    axes[0].tick_params(axis='x', rotation=45)\n","    \n","    # 誤差の大きいデータセットの分布をプロット\n","    sns.countplot(data=top_n_train_df_with_predictions, x=col, order=top_n_train_df_with_predictions[col].value_counts().index, ax=axes[1])\n","    axes[1].set_title(f'Distribution of {col} in Top {len(top_n_train_df_with_predictions)} Data Points with Largest Prediction Error')\n","    axes[1].tick_params(axis='x', rotation=45)\n","\n","    plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T14:17:23.971222Z","iopub.status.busy":"2023-08-21T14:17:23.970813Z","iopub.status.idle":"2023-08-21T14:17:24.013131Z","shell.execute_reply":"2023-08-21T14:17:24.011774Z","shell.execute_reply.started":"2023-08-21T14:17:23.971191Z"},"trusted":true},"outputs":[],"source":["top_n_train_df_with_predictions.query(\"price / prediction > 1\")\n","# top_n_train_df_with_predictions.query(\"price >  10000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["negative_difference_df = top_n_train_df_with_predictions[top_n_train_df_with_predictions['prediction'] - top_n_train_df_with_predictions['price'] > 0]\n","negative_difference_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def analyze_smallest_outliers(n_points=20):\n","    # 予測値と実際の値との差異を計算（MAPE）\n","    error_df = pd.DataFrame()\n","    error_df['prediction'] = oof_pred\n","    error_df['true_price'] = train_df.copy()['price']\n","    error_df['percentage_error'] = abs(error_df['true_price'] - error_df['prediction']) / error_df['true_price'] * 100 / 27532\n","\n","    # 差異が小さいデータポイントを取得\n","    small_error_df = error_df.nsmallest(n_points, 'percentage_error')\n","\n","    # グラフの幅をデータポイントの数に応じて調整\n","    width = max(15, n_points * 0.05)\n","\n","    # 差異の小さいデータポイントを可視化\n","    plt.figure(figsize=(width, 5))\n","    sns.barplot(x=small_error_df.index, y=small_error_df['percentage_error'], order=small_error_df.index)\n","    plt.xlabel('Index')\n","    plt.ylabel('Percentage Error')\n","    plt.title(f'Top {n_points} Data Points with Smallest Prediction Error (MAPE)')\n","    plt.show()\n","\n","    # 誤差が小さい順に指定した個数分のtrain_dfに予測値も追加して返す\n","    top_n_indices = small_error_df.index.values\n","    top_n_train_df = train_df.iloc[top_n_indices]\n","    top_n_train_df['prediction'] = small_error_df['prediction'].values\n","\n","    return top_n_train_df\n","\n","top_n_train_df_with_small_predictions = analyze_smallest_outliers(n_points=2000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["top_n_train_df_with_small_predictions.query(\"price < 10000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["negative_difference_df_small = top_n_train_df_with_small_predictions[top_n_train_df_with_small_predictions['prediction'] - top_n_train_df_with_small_predictions['price'] < 0]\n","negative_difference_df_small"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas_profiling\n","top_n_train_df_with_predictions.profile_report()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","top_n_train_df_with_small_predictions.profile_report()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_df.query(\"price < 3000\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def analyze_price_mape_relationship(bins=10):\n","    # 予測値と実際の値との差異を計算（MAPE）\n","    error_df = pd.DataFrame()\n","    error_df['prediction'] = oof_lgbm\n","    error_df['true_price'] = train_df.copy()['price']\n","    error_df['percentage_error'] = abs(error_df['true_price'] - error_df['prediction']) / error_df['true_price'] * 100\n","\n","    # 価格帯の範囲をデータ点の個数で均等に分割\n","    price_bins = pd.qcut(error_df['true_price'], q=bins)\n","    error_df['price_bin'] = price_bins\n","    price_error_grouped = error_df.groupby('price_bin')['percentage_error'].agg(['mean', 'std']).reset_index()\n","\n","    # グラフ描画\n","    plt.figure(figsize=(15, 5))\n","    sns.barplot(x='price_bin', y='mean', data=price_error_grouped, yerr=price_error_grouped['std'])\n","    plt.xlabel('Price Bin')\n","    plt.ylabel('Average MAPE')\n","    plt.title('Relationship between Price Band and Prediction Error (MAPE)')\n","    plt.xticks(rotation=45)\n","    plt.show()\n","\n","analyze_price_mape_relationship(bins=20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def analyze_price_mape_relationship_boxplot(bins=10):\n","    # 予測値と実際の値との差異を計算（MAPE）\n","    error_df = pd.DataFrame()\n","    error_df['prediction'] = oof_lgbm\n","    error_df['true_price'] = train_df.copy()['price']\n","    error_df['percentage_error'] = abs(error_df['true_price'] - error_df['prediction']) / error_df['true_price'] * 100\n","\n","    # 価格帯の範囲をデータ点の個数で均等に分割\n","    price_bins = pd.qcut(error_df['true_price'], q=bins)\n","    error_df['price_bin'] = price_bins\n","\n","    # 箱ひげ図描画\n","    plt.figure(figsize=(15, 5))\n","    sns.boxplot(x='price_bin', y='percentage_error', data=error_df)\n","    plt.xlabel('Price Bin')\n","    plt.ylabel('MAPE')\n","    plt.title('Relationship between Price Band and Prediction Error (MAPE)')\n","    plt.xticks(rotation=45)\n","    plt.show()\n","\n","analyze_price_mape_relationship_boxplot(bins=20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T12:46:32.940597Z","iopub.status.busy":"2023-08-21T12:46:32.940173Z","iopub.status.idle":"2023-08-21T12:46:53.845323Z","shell.execute_reply":"2023-08-21T12:46:53.844424Z","shell.execute_reply.started":"2023-08-21T12:46:32.940566Z"},"trusted":true},"outputs":[],"source":["# train_feat_df.info()\n","def plot_boxplot_for_categories(df, category_columns, threshold=20):\n","    # 予測値と実際の値との差異を計算（MAPE）\n","    error_df = pd.DataFrame()\n","    error_df['prediction'] = oof_lgbm\n","    error_df['true_price'] = train_df['price']\n","    error_df['percentage_error'] = abs(error_df['true_price'] - error_df['prediction']) / error_df['true_price'] * 100\n","\n","    # 指定したカテゴリ変数を追加\n","    for col in category_columns:\n","        error_df[col] = df[col]\n","\n","    # 各カテゴリ変数に対して箱ひげ図を描画\n","    for col in category_columns:\n","        # カテゴリのデータ数を取得\n","        category_counts = error_df[col].value_counts()\n","\n","        # カテゴリラベルにデータ数を追加\n","        error_df[col + '_label'] = error_df[col].apply(lambda x: str(x) + f' (n={category_counts[x]})')\n","\n","        if col != 'ori__year':\n","        # カテゴリをデータ数が多い順に並び替え\n","            category_order = category_counts.index.tolist()\n","            error_df[col + '_label'] = pd.Categorical(error_df[col + '_label'], categories=[str(cat) + f' (n={category_counts[cat]})' for cat in category_order], ordered=True)\n","            \n","        # 'year'の場合、カテゴリを数値に変換して並び替え\n","        if col == 'ori__year':\n","            # カテゴリラベルの一意なリストを作成\n","            unique_labels = sorted(set(error_df[col + '_label']), key=lambda x: int(float(x.split(' ')[0])))\n","            \n","            # 一意なリストを順序付きカテゴリとして使用\n","            error_df[col + '_label'] = pd.Categorical(\n","                error_df[col + '_label'],\n","                categories=unique_labels,\n","                ordered=True\n","            )\n","        if len(category_counts) > threshold:\n","            plt.figure(figsize=(10, 15))\n","            sns.boxplot(x='percentage_error', y=col + '_label', data=error_df, orient='h')\n","            # 平均値を描画\n","            sns.pointplot(x='percentage_error', y=col + '_label', data=error_df, color='red', markers='o', linestyles='--', join=False, orient='h') # orient='h'で水平に描画\n","            plt.ylabel(col)\n","            plt.xlabel('MAPE')\n","            plt.xlim(0, 80)\n","        else:\n","            plt.figure(figsize=(15, 5))\n","            sns.boxplot(x=col + '_label', y='percentage_error', data=error_df)\n","            # 平均値を描画\n","            sns.pointplot(x=col + '_label', y='percentage_error', data=error_df, color='red', markers='o', linestyles='--', join=False)\n","            plt.xlabel(col)\n","            plt.ylabel('MAPE')\n","            plt.ylim(0, 80)\n","        \n","        plt.title(f'Relationship between {col} and Prediction Error (MAPE)')\n","        plt.xticks(rotation=45 if len(category_counts) <= threshold else 0)\n","        plt.show()\n","# 例: 'ori__odometer'を10のビンに分割\n","bins = 20\n","train_feat_df_23 = train_feat_df.copy()\n","train_feat_df_23['ori__odometer_binned'] = pd.qcut(train_feat_df_23['ori__odometer'], q=bins)\n","\n","category_columns = [\n","    'ori__cylinders',\n","    'ori__manufacturer',\n","    'ori__condition',\n","    'ori__fuel',\n","    'ori__title_status',\n","    'ori__transmission',\n","    'ori__drive',\n","    'ori__size',\n","    'ori__type',\n","    'ori__paint_color',\n","    'ori__state',\n","    'ori__year',\n","    'ori__odometer_binned'\n","]\n","plot_boxplot_for_categories(train_feat_df_23, category_columns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-21T12:04:45.896007Z","iopub.status.busy":"2023-08-21T12:04:45.895253Z","iopub.status.idle":"2023-08-21T12:06:11.478315Z","shell.execute_reply":"2023-08-21T12:06:11.477318Z","shell.execute_reply.started":"2023-08-21T12:04:45.895978Z"},"trusted":true},"outputs":[],"source":["# # 最良の特徴量セットで全訓練データを使用して訓練\n","final_model = lgb.LGBMRegressor(**lgbm_params)\n","final_model.fit(\n","    train_feat_df,\n","    train_df[\"price\"],\n","    categorical_feature=[],\n",")\n","#これを推論に使ってはいけない\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # 訓練データ全体での評価\n","pred_lgbm_train = final_model.predict(train_feat_df)\n","whole_score = mean_absolute_percentage_error(train_df[\"price\"], pred_lgbm_train)\n","print(f\"Whole Training Data MAPE: {whole_score:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def optimize_features_by_importance_and_cv(X, y, cv, params=None, verbose=False):\n","    if params is None:\n","        params = {}\n","\n","    # 初期設定\n","    features = list(X.columns)\n","    best_score = float('inf')\n","    best_features = features.copy()\n","\n","    while len(features) > 1:\n","        # 特徴量セットでのモデル訓練\n","        score, _, models = fit_lgbm(X[features], y, cv, params=params, verbose=verbose)\n","\n","        # 現在のスコアが最良なら更新\n","        if score < best_score:\n","            best_score = score\n","            best_features = features.copy()\n","\n","        # 特徴量重要度の計算\n","        feature_importance = np.mean([model.feature_importances_ for model in models], axis=0)\n","\n","        # 最も重要度が低い特徴量の削除\n","        least_important_idx = np.argmin(feature_importance)\n","        del features[least_important_idx]\n","\n","        if verbose:\n","            print(f\"Removed feature: {features[least_important_idx]}. Number of features left: {len(features)}\")\n","\n","    if verbose:\n","        print(f\"Best score: {best_score}. Number of best features: {len(best_features)}\")\n","\n","    return best_features\n","\n","# 特徴量の最適化を実行\n","best_features = optimize_features_by_importance_and_cv(train_feat_df, train_df[\"price\"], cv, params=lgbm_params, verbose=True)\n","\n","# 最良の特徴量セットで再訓練\n","score_lgbm, oof_lgbm, models_lgbm = fit_lgbm(\n","    train_feat_df[best_features],\n","    train_df[\"price\"],\n","    categorical_cols=[],\n","    params=lgbm_params,\n","    cv=cv,\n","    verbose=False,\n",")\n","\n","# 推論\n","pred_lgbm = np.array([model.predict(test_feat_df[best_features]) for model in models_lgbm])\n","pred_lgbm = np.mean(pred_lgbm, axis=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # n_fold = 5\n","# # lgbm_params = {\n","# #     \"objective\": \"mape\",\n","# #     \"metrics\": \"mape\",\n","# #     \"n_estimators\": 10000,\n","# #     \"learning_rate\": 0.01,\n","# #     \"max_depth\": 5,\n","# #     \"num_leaves\": 32,\n","# #     \"colsample_bytree\": 0.446,\n","# #     \"subsample\": 0.339,\n","# #     \"importance_type\": \"gain\",\n","# #     \"random_state\": 88,\n","# # }\n","\n","# feat_cat_cols = train_feat_df.select_dtypes(include=\"category\").columns.tolist()\n","\n","# cv = get_cv(train_feat_df, n_fold=5)\n","\n","# #optuna\n","# best_params_lgbm = tuning_lgbm(\n","#     train_feat_df.copy(),\n","#     train_df,\n","#     cv=cv,\n","# #     categorical_cols = feat_cat_cols\n","# )\n","# # training\n","# score_lgbm, oof_lgbm, models_lgbm = fit_lgbm(\n","#     train_feat_df,\n","#     y=train_df[\"price\"],\n","#     categorical_cols=[],\n","#     params=best_params_lgbm,\n","#     cv=cv,\n","#     verbose=False,\n","# )\n","\n","# # inference\n","# pred_lgbm = np.array([model.predict(test_feat_df) for model in models_lgbm])\n","# pred_lgbm = np.mean(pred_lgbm, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# training\n","best_params_cat = tuning_cat(\n","    train_feat_df.copy(),\n","    y=train_df[\"price\"],\n","    cv=cv,\n","     categorical_cols = feat_cat_cols\n",")\n","\n","score_cat, oof_cat, models_cat = fit_cat(\n","    X, y, cv=cv, categorical_cols=feat_cat_cols, params=best_params_cat, verbose=False)\n","\n","# テストデータの予測\n","test_feat_df = convert_categorical_features(test_feat_df, feat_cat_cols)\n","\n","# inference\n","pred_cat = np.array([model.predict(test_feat_df) for model in models_cat])\n","pred_cat = np.mean(pred_cat, axis=0)\n","\n","pred = (pred_lgbm + pred_cat)/2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["feat_cat_cols"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_feat_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# One-Hotエンコーディング\n","state_dummies = pd.get_dummies(train_df['state'], prefix='state')\n","train_df = pd.concat([train_df, state_dummies], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","# t-SNEの実行\n","tsne = TSNE(n_components=2, random_state=42)\n","X_tsne = tsne.fit_transform(train_feat_df)\n","\n","# 可視化\n","plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=state_labels, cmap='viridis')\n","plt.xlabel('t-SNE Component 1')\n","plt.ylabel('t-SNE Component 2')\n","plt.colorbar(label='State')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.decomposition import PCA\n","\n","def preprocess_and_visualize(df):\n","    # 0. stateカラムのエンコーディング\n","    le = LabelEncoder()\n","    df['ori__manufacturer'] = le.fit_transform(df['ori__manufacturer'])\n","\n","    # 1. 他のカテゴリカルデータのエンコーディング\n","    df_dummies = pd.get_dummies(df, columns=df.select_dtypes(include='category').columns.tolist())\n","\n","    # 2. 欠損値の処理\n","    df_dummies.fillna(df_dummies.median(numeric_only=True), inplace=True)\n","\n","    # 3. 標準化\n","    scaler = StandardScaler()\n","    df_scaled = scaler.fit_transform(df_dummies)\n","\n","    # 4. 次元削減\n","    pca = PCA(n_components=2) # 2次元に削減\n","    df_pca = pca.fit_transform(df_scaled)\n","\n","    # 可視化\n","    plt.figure(figsize=(10, 8))\n","    plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df['ori__manufacturer'], cmap='viridis')\n","    plt.title('PCA Visualization by manufacturer')\n","    plt.xlabel('Principal Component 1')\n","    plt.ylabel('Principal Component 2')\n","    plt.colorbar(label='manufacturer')\n","    plt.show()\n","\n","# 関数を呼び出して可視化\n","preprocess_and_visualize(train_feat_df.copy())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# train_feat_df.drop([\"price\"], axis=1)\n","train_feat_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.optimize import minimize\n","\n","# 重みを最適化する関数\n","def ensemble_weights_optimization(oof_preds, y):\n","    def ensemble_error(weights):\n","        ensemble_pred = sum([weight * oof_pred for weight, oof_pred in zip(weights, oof_preds)])\n","        return mean_absolute_percentage_error(y, ensemble_pred)\n","\n","    cons = ({'type': 'eq', 'fun': lambda w: 1 - sum(w)})\n","    bounds = [(0, 1)] * len(oof_preds)\n","    result = minimize(ensemble_error, [1 / len(oof_preds)] * len(oof_preds), method='SLSQP', bounds=bounds, constraints=cons)\n","    return result.x\n","\n","# アンサンブルのメイン関数\n","def ensemble_lgbm(train_feat_df, test_feat_df, y, cv, seed_range, max_depth_range, num_leaves_range, colsample_bytree_range, subsample_range):\n","    oof_preds_by_seed = []\n","    test_preds_by_seed = []\n","\n","    for seed in seed_range:\n","        oof_preds = []\n","        test_preds = []\n","        for max_depth in max_depth_range:\n","            for num_leaves in num_leaves_range:\n","                for colsample_bytree in colsample_bytree_range:\n","                    for subsample in subsample_range:\n","                        params = {\n","                            \"objective\": \"mape\",\n","                            \"metrics\": \"mape\",\n","                            \"n_estimators\": 10000,\n","                            \"learning_rate\": 0.01,\n","                            \"max_depth\": max_depth,\n","                            \"num_leaves\": num_leaves,\n","                            \"colsample_bytree\": colsample_bytree,\n","                            \"subsample\": subsample,\n","                            \"importance_type\": \"gain\",\n","                            \"random_state\": seed,\n","                        }\n","                        score_lgbm, oof_lgbm, models_lgbm = fit_lgbm(\n","                            train_feat_df,\n","                            y=y,\n","                            categorical_cols=[],\n","                            params=params,\n","                            cv=cv,\n","                            verbose=False,\n","                        )\n","                        pred_lgbm = np.array([model.predict(test_feat_df) for model in models_lgbm])\n","                        oof_preds.append(oof_lgbm)\n","                        test_preds.append(np.mean(pred_lgbm, axis=0))\n","\n","        # このseedでの最適な重みでアンサンブル\n","        weights = ensemble_weights_optimization(oof_preds, y)\n","        final_oof_pred_seed = np.zeros_like(oof_preds[0])\n","        final_test_pred_seed = np.zeros_like(test_preds[0])\n","        for weight, oof_pred, test_pred in zip(weights, oof_preds, test_preds):\n","            final_oof_pred_seed += weight * oof_pred\n","            final_test_pred_seed += weight * test_pred\n","\n","        oof_preds_by_seed.append(final_oof_pred_seed)\n","        test_preds_by_seed.append(final_test_pred_seed)\n","\n","    # seedに対して平均を取る\n","    final_oof_pred = np.mean(oof_preds_by_seed, axis=0)\n","    final_test_pred = np.mean(test_preds_by_seed, axis=0)\n","\n","    return final_oof_pred, final_test_pred\n","\n","# ハイパーパラメータの範囲\n","seed_range = [88, 42]#, 123\n","max_depth_range = [5, 6, 7]\n","num_leaves_range = [32]#, 64\n","colsample_bytree_range = [0.4, 0.5]\n","subsample_range = [0.3, 0.4]\n","\n","# アンサンブル関数の呼び出し\n","final_oof_pred, final_test_pred = ensemble_lgbm(\n","    train_feat_df,\n","    test_feat_df,\n","    y=train_df[\"price\"],\n","    cv=cv,\n","    seed_range=seed_range,\n","    max_depth_range=max_depth_range,\n","    num_leaves_range=num_leaves_range,\n","    colsample_bytree_range=colsample_bytree_range,\n","    subsample_range=subsample_range\n",")\n","\n","# 結果の確認\n","score = mean_absolute_percentage_error(train_df[\"price\"], final_oof_pred)\n","print(f\"Final Ensemble Score: {score:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from scipy.optimize import minimize\n","\n","# def ensemble_error(weights):\n","#     ensemble_pred = weights[0] * oof_cat + weights[1] * oof_lgbm\n","#     return mean_absolute_percentage_error(train_df[\"price\"], ensemble_pred)\n","\n","# cons = ({'type': 'eq', 'fun': lambda w: 1 - sum(w)})\n","# bounds = [(0, 1)] * 2\n","\n","# # 初期値の候補を用意\n","# initial_weights_candidates = [\n","#     [0.1, 0.9], [0.3, 0.7], [0.5, 0.5], [0.7, 0.3], [0.9, 0.1],\n","# #     [0.02, 0.98], [0.04, 0.96], \n","#     [0.06, 0.94], [0.08, 0.92], [0.12, 0.88]]\n","\n","# # 最良の結果を保存する変数\n","# best_result = None\n","# best_error = float('inf')\n","\n","# # 初期値ごとに最適化を行う\n","# for initial_weights in initial_weights_candidates:\n","#     result = minimize(ensemble_error, initial_weights, method='SLSQP', bounds=bounds, constraints=cons)\n","#     print(f\"Initial weights: {initial_weights}, MAPE: {result.fun}, Success: {result.success}\")\n","#     if result.success and result.fun < best_error:\n","#         best_error = result.fun\n","#         best_result = result\n","\n","# # 最良の結果を表示\n","# print(\"Best result:\")\n","# print(best_result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","def visualize_by_category(df):\n","    # 数値変数だけを取り出す\n","    numerical_df = df.select_dtypes(exclude='category')\n","\n","    # 欠損値の処理\n","    numerical_df.fillna(numerical_df.median(), inplace=True)\n","\n","    # 標準化\n","    scaler = StandardScaler()\n","    numerical_scaled = scaler.fit_transform(numerical_df)\n","\n","    # カテゴリカル変数のリストを取得\n","    categorical_columns = df.select_dtypes(include='category').columns.tolist()\n","\n","    # カテゴリカル変数ごとに処理\n","    for col in categorical_columns:\n","        # 次元削減\n","        pca = PCA(n_components=2)\n","        pca_result = pca.fit_transform(numerical_scaled)\n","\n","        # カテゴリカル変数の値を取り出す\n","        category_labels = df[col].astype(str)\n","\n","        # 可視化用のデータフレームを作成\n","        plot_df = pd.DataFrame({\n","            'Principal Component 1': pca_result[:, 0],\n","            'Principal Component 2': pca_result[:, 1],\n","            col: category_labels\n","        })\n","\n","        # 可視化\n","        plt.figure(figsize=(10, 8))\n","        sns.scatterplot(x='Principal Component 1', y='Principal Component 2', hue=col, data=plot_df, palette='viridis')\n","        plt.title(f'PCA Visualization by {col}')\n","        plt.show()\n","\n","# 関数を呼び出して可視化\n","visualize_by_category(train_feat_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
